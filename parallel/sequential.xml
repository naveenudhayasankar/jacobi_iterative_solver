<?xml version="1.0" encoding="iso-8859-1"?>
<ipm_job_profile>
<calltable nsections="1" >
<section module="MPI" nentries="69" >
<entry name="MPI_Init" />
<entry name="MPI_Init_thread" />
<entry name="MPI_Finalize" />
<entry name="MPI_Comm_rank" />
<entry name="MPI_Comm_size" />
<entry name="MPI_Send" />
<entry name="MPI_Ssend" />
<entry name="MPI_Rsend" />
<entry name="MPI_Bsend" />
<entry name="MPI_Isend" />
<entry name="MPI_Issend" />
<entry name="MPI_Irsend" />
<entry name="MPI_Ibsend" />
<entry name="MPI_Recv" />
<entry name="MPI_Irecv" />
<entry name="MPI_Sendrecv" />
<entry name="MPI_Sendrecv_replace" />
<entry name="MPI_Wait" />
<entry name="MPI_Waitany" />
<entry name="MPI_Waitall" />
<entry name="MPI_Waitsome" />
<entry name="MPI_Probe" />
<entry name="MPI_Iprobe" />
<entry name="MPI_Send_init" />
<entry name="MPI_Ssend_init" />
<entry name="MPI_Rsend_init" />
<entry name="MPI_Bsend_init" />
<entry name="MPI_Recv_init" />
<entry name="MPI_Buffer_attach" />
<entry name="MPI_Buffer_detach" />
<entry name="MPI_Test" />
<entry name="MPI_Testany" />
<entry name="MPI_Testall" />
<entry name="MPI_Testsome" />
<entry name="MPI_Start" />
<entry name="MPI_Startall" />
<entry name="MPI_Bcast" />
<entry name="MPI_Reduce" />
<entry name="MPI_Reduce_scatter" />
<entry name="MPI_Barrier" />
<entry name="MPI_Gather" />
<entry name="MPI_Gatherv" />
<entry name="MPI_Scatter" />
<entry name="MPI_Scatterv" />
<entry name="MPI_Scan" />
<entry name="MPI_Allgather" />
<entry name="MPI_Allgatherv" />
<entry name="MPI_Allreduce" />
<entry name="MPI_Alltoall" />
<entry name="MPI_Alltoallv" />
<entry name="MPI_Comm_group" />
<entry name="MPI_Comm_compare" />
<entry name="MPI_Comm_dup" />
<entry name="MPI_Comm_create" />
<entry name="MPI_Comm_split" />
<entry name="MPI_Comm_free" />
<entry name="MPI_Ibcast" />
<entry name="MPI_Ireduce" />
<entry name="MPI_Ireduce_scatter" />
<entry name="MPI_Igather" />
<entry name="MPI_Igatherv" />
<entry name="MPI_Iscatter" />
<entry name="MPI_Iscatterv" />
<entry name="MPI_Iscan" />
<entry name="MPI_Iallgather" />
<entry name="MPI_Iallgatherv" />
<entry name="MPI_Iallreduce" />
<entry name="MPI_Ialltoall" />
<entry name="MPI_Ialltoallv" />
</section>
</calltable>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="0" mpi_size="1" stamp_init="1670300109.187761" stamp_final="1670300139.307213" username="naveen" allocationname="unknown" flags="0" pid="28979" >
<job nhosts="1" ntasks="1" start="1670300109" final="1670300139" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >naveen-VirtualB</host>
<perf wtime="3.01195e+01" utime="2.16917e+01" stime="8.19668e+00" mtime="1.21317e-02" gflop="0.00000e+00" gbyte="5.46227e-02" omp_num_threads="1"></perf>
<modules nmod="1">
<module name="MPI" time="1.21317e-02" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/home/naveen/cse708/jacobi_iteration/parallel/mpi_jacobi" md5sum="c3b4f4320000377f377f377fe955377fdf" >./mpi_jacobi 1000 </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="3.01175e+01" utime="2.16898e+01" stime="8.19668e+00" mtime="1.21317e-02" id="0">
<modules nmod="1">
<module name="MPI" time="1.21317e-02" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="1" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Allgather" count="18200" bytes="6.5302e+07" > 1.2131e-02 </func>
</region>
</regions>
<internal rank="0" log_i="1670300139.307213" log_t="1.6703e+09" report_delta="-1.0000e+00" fname="./naveen.1670300109.187761.ipm.xml" logrank="0" ></internal>
</task>
</ipm_job_profile>
